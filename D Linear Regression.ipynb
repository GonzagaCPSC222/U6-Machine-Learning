{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 222](https://github.com/GonzagaCPSC222) Intro to Data Science\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Linear Regression\n",
    "What are our learning objectives for this lesson?\n",
    "* Calculate a least squares linear regression line\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "* Open LinearRegression.ipynb from last class\n",
    "* For our x, y scatter data, compute the correlation coefficient $r$\n",
    "$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "    * Add more/less noise to our y values. How does this affect $r$?\n",
    "* Read the rest of the Linear Regression notes below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Attendance\n",
    "* Announcements\n",
    "    * Great mid-project demos\n",
    "    * IQ9 is graded, average was ~8/10, fantastic job!\n",
    "    * DA7 questions?\n",
    "* Today\n",
    "    * Finish the kNN.ipynb\n",
    "        * Quick overview of bootstrap method\n",
    "        * \"Classifier Evaluation Metrics\"\n",
    "            * Including confusion matrices\n",
    "    * Start LinearRegression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "In supervised machine learning, when your \"class\" attribute is continuous, the machine learning task is called regression instead of classification. There are several regression algorithms in Sci-kit Learn that can be used for these tasks, such as:\n",
    "* [Linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* [kNN regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "* [Decision tree regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "* [Support vector regressor](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "* Etc.\n",
    "\n",
    "While these algorithms can get complex, it is pretty straightforward to implement our own \"simple\" linear regression algorithm. To do this, consider regression tasks where we have one feature variable (an independent variable, x) and one target variable (a dependent variable, y). Given a training set of (x, y) pairs, we can fit a line y = mx + b that can be used for unseen instances (e.g. x values) to predict a y value.\n",
    "\n",
    "## Simple Linear Regression\n",
    "In scatter plots of (x, y) data, it can be helpful to \"fit a line\"\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC222/U6-Machine-Learning/master/figures/linear_regression_example.png\" width=\"600\"/>\n",
    "\n",
    "* this can be done via linear regression\n",
    "* we're going to look at a simple approach called \"Least Squares\"\n",
    "\n",
    "The basic idea: Given a set of points, find a line that \"best\" fits the points\n",
    "* i.e., find values for $m$ (slope) and $b$ (intercept) that best fits $y = mx + b$\n",
    "\n",
    "In least squares linear regression\n",
    "* find $m$ and $b$ that minimizes the sum of the (vertical) squared distance to the measured data points\n",
    "* once we find $m$, finding $b$ isn't difficult\n",
    "\n",
    "The basic least squares approach:\n",
    "1. Calculate the mean $\\bar{x}$ of the $x$ values and the mean $\\bar{y}$ of the $y$ values\n",
    "    * note the line must go through the point ($\\bar{x}$, $\\bar{y}$)\n",
    "2. Calculate the slope using the means (where n is the number of data points):\n",
    "$$m = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "3. Calculate the y intercept as b = $\\bar{y} - m\\bar{x}$\n",
    "     * or, $\\bar{y} = m\\bar{x} + b$ ... since we know it must go through ($\\bar{x}$, $\\bar{y}$)\n",
    "     \n",
    "## Regression Evaluation Metrics\n",
    "The correlation coefficient $r$ helps checks how good the linear relationship is:\n",
    "$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "* Note the bottom is essentially the same as the top just squared to strip away\n",
    "the signs\n",
    "* If the correlation is perfectly linear, then result is 1\n",
    "* If the correlation is perfect inverse linear, then result is -1\n",
    "* If no relationship, the result is 0\n",
    "\n",
    "An alternative formula (where $\\sigma_x$ is the standard deviation of $x$):\n",
    "$$m = r \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "The coefficient of determination $R^2$ is the correlation coefficient squared, $R^2 = r^2$\n",
    "* $R^2$ is the proportion of variation in the dependent (y) variable that is explained by the independent (x) variable\n",
    "* The higher $R^2$, the better the fit line \n",
    "\n",
    "The covariance can also be used to assess correlation\n",
    "$$cov = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n}$$\n",
    "* covariance can also be used to calculate the correlation coefficient:\n",
    "$$r = \\frac{cov}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "The standard error (SE) is also used to help check the fit\n",
    "$$SE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - y^\\prime)^2}{n}}$$\n",
    "* Where $y^\\prime$ is the \"predicted\" value and $y$ is the actual value\n",
    "* $(y_i - y^\\prime)$ is called a \"residual\"\n",
    "* Note standard error is essentially the standard deviation of the differences: $SE = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "* Lower the value the \"better\" the fit\n",
    "\n",
    "Additional regression evaluation metrics that are commonly used to quantify the residuals:\n",
    "* Mean absolute error (MAE)\n",
    "* Mean square error (MSE)\n",
    "* Root mean square error (RMSE)\n",
    "* Normalized RMSE (NRMSE)\n",
    "* Etc. \n",
    "\n",
    "Q: What does it mean if there is a strong (linear) correlation?\n",
    "* one of the attributes is (potentially) redundant because it is implied by the other\n",
    "* one is a good predictor for the other ... good if one is a class label\n",
    "* i.e., regression is one way to make predictions (more later)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
